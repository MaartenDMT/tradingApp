    
    self.model = self.get_model(
        hidden_units, dropout, m_activation, self.modelname)

    for layer in self.model.layers:
        if hasattr(layer, 'get_weights'):
            rl_logger.info(f'Layer: {layer.name}')

    agent_logger.info(f"Agent initialized with parameters:\n"
                        f"gamma: {gamma}, hidden_units: {hidden_units}, opt: {opt}, learning_rate: {learning_rate}, epsilon: {epsilon},\n "
                        f"epsilon_min: {epsilon_min}, epsilon_decay: {epsilon_decay}, ")

    def get_model(self, hidden_units, dropout, m_activation, chosen_model):
        models = {
            "Standard_Model": self._build_model(hidden_units, dropout, m_activation),
            "Dense_Model": self.base_dense_model(m_activation, dropout),
            "LSTM_Model": self.base_lstm_model(m_activation, dropout),
            "CONV1D_LSTM_Model": self.base_conv1d_lstm_model(m_activation, dropout),
            "build_resnet_model": self.build_resnet_model(m_activation, dropout),
            "base_conv1d_model": self.base_conv1d_model(m_activation, dropout),
            "base_transformer_model": self.base_transformer_model(m_activation, dropout),
        }

        return models[chosen_model]


    def get_model_parameters(self):
        """
        Retrieve the parameters used to construct the model.
        """
        return {
            "model_name": self.modelname,
            "gamma": self.gamma,
            "hidden_units": self.hidden_units,
            "learning_rate": self.learning_rate,
            "epsilon": self.epsilon,
            "dropout": self.dropout,
            "act": self.act,
            "m_activation": self.m_activation,
            "input_dim": self.osn,
            "action_space_n": self.env.action_space.n,
            "env_actions": self.env_actions,
            "loss": self.loss,
            "learning_rate": self.learning_rate,
            "metrics": ['accuracy']
        }

    
    def base_dense_model(self, m_activation, dropout):
        agent_logger.info("Dense Model loaded.")
        base_model = tf.keras.Sequential(name='DenseModel')
        base_model.add(tf.keras.layers.Dense(
            128, input_dim=self.osn, activation='relu'))
        base_model.add(BatchNormalization())
        base_model.add(Dropout(dropout))
        base_model.add(tf.keras.layers.Dense(64, activation='relu'))
        base_model.add(BatchNormalization())
        base_model.add(tf.keras.layers.Dense(32, activation='relu'))
        base_model.add(BatchNormalization())
        base_model.add(tf.keras.layers.Dense(
            self.env_actions, activation=m_activation))
        base_model.compile(
            optimizer=self.opt(learning_rate=self.learning_rate), loss=self.loss, metrics=['accuracy'])

        return base_model


    def _build_model(self, hidden_units, dropout, m_activation):
        rl_logger.info("Standard Model loaded.")
        model = tf.keras.Sequential(name='StandardModel')
        model.add(tf.keras.layers.Dense(64, input_dim=self.osn,
                  activation='relu', kernel_regularizer=l2(0.01)))
        model.add(BatchNormalization())
        model.add(Dropout(dropout))
        model.add(tf.keras.layers.Dense(
            hidden_units, activation='relu', kernel_regularizer=l2(0.01)))
        model.add(BatchNormalization())
        model.add(tf.keras.layers.Dense(
            self.env_actions, activation=m_activation))
        model.compile(loss=self.loss, optimizer=self.opt(
            learning_rate=self.learning_rate), metrics=['accuracy'])
        agent_logger.info(
            f"Model built with action space {self.env.action_space.n}.")
        return model


    def base_lstm_model(self, m_activation, dropout):
        rl_logger.info("LSTM Model loaded.")
        base_model = tf.keras.Sequential(name='LSTMModel')
        base_model.add(tf.keras.layers.LSTM(100, activation='relu', input_shape=(
            self.osn, 1), return_sequences=True))
        base_model.add(tf.keras.layers.Dropout(dropout))
        base_model.add(tf.keras.layers.LSTM(100, activation='relu'))
        base_model.add(tf.keras.layers.Dense(
            self.env_actions, activation=m_activation))
        base_model.compile(
            optimizer=self.opt(learning_rate=self.learning_rate), loss=self.loss, metrics=['accuracy'])

        return base_model

    def base_conv1d_lstm_model(self, m_activation, dropout):
        rl_logger.info("CONV1D Model loaded.")
        base_model = tf.keras.Sequential(name='CONV1DModel')
        base_model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=2,
                       activation='relu', input_shape=(self.osn, 1)))
        base_model.add(tf.keras.layers.MaxPool1D(pool_size=1))
        base_model.add(tf.keras.layers.LSTM(
            100, activation='relu', return_sequences=True))
        base_model.add(tf.keras.layers.Dropout(dropout))
        base_model.add(tf.keras.layers.LSTM(100, activation='relu'))
        base_model.add(tf.keras.layers.Dense(
            self.env_actions, activation=m_activation))
        base_model.compile(
            optimizer=self.opt(learning_rate=self.learning_rate), loss=self.loss, metrics=['accuracy'])

        return base_model

    def build_resnet_model(self, m_activation, dropout):
        rl_logger.info("ResNet Model loaded.")

        inputs = tf.keras.layers.Input(shape=(self.osn,))
        x = tf.keras.layers.Dense(128, activation='relu')(inputs)
        x = BatchNormalization()(x)
        x = Dropout(dropout)(x)

        # Residual block
        residual = x
        x = tf.keras.layers.Dense(128, activation='relu')(
            x)  # Change this to 128 units
        x = BatchNormalization()(x)
        x = Dropout(dropout)(x)
        x = tf.keras.layers.Dense(128, activation='relu')(
            x)  # Change this to 128 units
        x = tf.keras.layers.Add()([x, residual])

        x = BatchNormalization()(x)
        outputs = tf.keras.layers.Dense(
            self.env_actions, activation=m_activation)(x)

        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer=self.opt(
            learning_rate=self.learning_rate), loss=self.loss, metrics=['accuracy'])
        return model

    def base_conv1d_model(self, m_activation, dropout):
        rl_logger.info("Simple CONV1D Model loaded.")

        base_model = tf.keras.Sequential(name='SimpleCONV1D')
        base_model.add(tf.keras.layers.Conv1D(
            filters=64, kernel_size=2, activation='relu', input_shape=(self.osn, 1)))
        base_model.add(tf.keras.layers.MaxPool1D(pool_size=1))
        base_model.add(tf.keras.layers.Flatten())
        base_model.add(tf.keras.layers.Dense(100, activation='relu'))
        base_model.add(tf.keras.layers.Dropout(dropout))
        base_model.add(tf.keras.layers.Dense(
            self.env_actions, activation=m_activation))

        base_model.compile(optimizer=self.opt(
            learning_rate=self.learning_rate), loss=self.loss, metrics=['accuracy'])
        return base_model

    def base_transformer_model(self, m_activation, dropout=0.1, d_model=128, num_heads=4):
        rl_logger.info("Transformer Model loaded.")

        inputs = tf.keras.layers.Input(shape=(self.osn, 1))
        x = tf.keras.layers.Dense(d_model)(inputs)

        # Embed the sequence into the d_model space
        x = tf.keras.layers.Dense(d_model)(x)

        # Add positional encoding if needed. Skipped in this example for brevity.
        # x = positional_encoding(x, d_model)

        x = transformer_block(d_model, num_heads, dropout)(x)
        x = tf.keras.layers.GlobalAveragePooling1D()(x)
        outputs = tf.keras.layers.Dense(
            self.env_actions, activation=m_activation)(x)

        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer=self.opt(
            learning_rate=self.learning_rate), loss=self.loss, metrics=['accuracy'])
        return model